{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dcc22e8b",
      "metadata": {
        "id": "dcc22e8b"
      },
      "source": [
        "# Análise de Embeddings e Redução da Dimensionalidade\n",
        "\n",
        "**Objetivo.** Dado um conjunto de textos, gerar embeddings com BERT e investigar a estrutura dos dados via PCA, t-SNE e UMAP. Em seguida, identificar clusters e relacioná-los a categorias semânticas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3272b0d4",
      "metadata": {
        "id": "3272b0d4"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    'I swap butter for olive oil in many recipes.',\n",
        "    'Canberra is the capital of Australia.',\n",
        "    'Ottawa is the capital city of Canada.',\n",
        "    'Paris is the most populated city in France.',\n",
        "    'Tokyo is among the most populous metropolitan areas worldwide.',\n",
        "    'I prefer my coffee with no sugar and a splash of milk.',\n",
        "    'The recipe for pasta carbonara is simple.',\n",
        "    'A pinch of salt enhances sweetness in desserts.',\n",
        "    'Alignment techniques reduce harmful outputs.',\n",
        "    'Explainable AI highlights salient features for decisions.',\n",
        "    'Transformer models enable long-range language dependencies.',\n",
        "    'Black swan events stress-test portfolio resilience.',\n",
        "    'The Sahara Desert spans much of North Africa.',\n",
        "    'Inflation erodes real purchasing power of cash.',\n",
        "    'Aromatics like garlic and onion build flavor early.',\n",
        "    'Value stocks trade at lower multiples relative to fundamentals.',\n",
        "    'Quantization reduces memory with minimal accuracy loss.',\n",
        "    'Tax-loss harvesting offsets capital gains.',\n",
        "    'Investing in technology can be risky.',\n",
        "    'Fermented foods add acidity and complexity.',\n",
        "    'Marinating tofu improves texture and taste.',\n",
        "    'Vector databases power semantic search at scale.',\n",
        "    'Distillation transfers knowledge from large to small models.',\n",
        "    'The Great Barrier Reef lies off Australia’s northeast coast.',\n",
        "    'Retrieval-augmented generation grounds answers in sources.',\n",
        "    'Iceland lies on the Mid-Atlantic Ridge.',\n",
        "    'The Baltic states border the eastern Baltic Sea.',\n",
        "    'Multimodal learning aligns text with images and audio.',\n",
        "    'Risk tolerance should guide position sizing.',\n",
        "    'Time in the market beats timing the market.',\n",
        "    'Behavioral biases can derail investment plans.',\n",
        "    'Reinforcement learning fine-tunes policies from human feedback.',\n",
        "    'Edge AI runs models under strict latency constraints.',\n",
        "    'Deglazing lifts browned bits to make pan sauces.',\n",
        "    'Tempering chocolate stabilizes cocoa butter crystals.',\n",
        "    'What is the capital of France?',\n",
        "    'Johannesburg is a major city but not South Africa’s capital.',\n",
        "    'The Danube passes through multiple European capitals.',\n",
        "    'The Amazon River carries one of the largest water volumes on Earth.',\n",
        "    'A healthy emergency fund reduces forced selling.',\n",
        "    'I batch-cook grains for quick lunches.',\n",
        "    'Resting steak helps redistribute the juices.',\n",
        "    'The Atacama is one of the driest deserts on the planet.',\n",
        "    'Liquidity risk rises when trading volumes are thin.',\n",
        "    'Mount Everest is the highest peak above sea level.',\n",
        "    'Graph neural networks capture relational structure.',\n",
        "    'Sourdough starter needs regular feedings to stay active.',\n",
        "    'The stock market experienced a drop today.',\n",
        "    'Umami-rich ingredients deepen savory dishes.',\n",
        "    'Al dente pasta retains a slight bite after cooking.',\n",
        "    'Rebalancing restores target asset allocation.',\n",
        "    'Continual learning mitigates catastrophic forgetting.',\n",
        "    'Bond duration measures sensitivity to interest-rate changes.',\n",
        "    'Diffusion models synthesize high-fidelity images.',\n",
        "    'Expense ratios compound against long-term returns.',\n",
        "    'Self-supervised pretraining reduces labeled data needs.',\n",
        "    'What country contains the city of Kyoto?',\n",
        "    'Stir-frying requires high heat and constant movement.',\n",
        "    'Covered calls generate income with capped upside.',\n",
        "    'The Nile flows northward into the Mediterranean Sea.',\n",
        "    'Causal inference distinguishes correlation from effect.',\n",
        "    'Prompt engineering steers generative behavior reliably.',\n",
        "    'Few-shot prompting improves generalization on new tasks.',\n",
        "    'Growth investing prioritizes earnings expansion.',\n",
        "    'The Alps stretch across several central European countries.',\n",
        "    'The Andes form a continuous mountain range along South America.',\n",
        "    'I cook vegetarian meals on weekdays to simplify planning.',\n",
        "    'Natural language processing has advanced greatly.',\n",
        "    'Sous-vide delivers precise temperature control.',\n",
        "    'Diversification reduces idiosyncratic risk across holdings.',\n",
        "    'Sharpe ratio evaluates risk-adjusted performance.',\n",
        "    'Artificial intelligence is transforming the world.',\n",
        "    'Credit spreads widen during economic uncertainty.',\n",
        "    'Emerging markets add diversification but higher volatility.',\n",
        "    'Mise en place speeds up weeknight cooking.',\n",
        "    'The Caspian Sea is a landlocked body of water.',\n",
        "    'Evaluation with benchmarks must avoid data leakage.',\n",
        "    'Cairo sits along the Nile River delta.',\n",
        "    'Federated learning trains models without centralizing data.',\n",
        "    'Lagos is Nigeria’s largest city by population.',\n",
        "    'Dollar-cost averaging smooths entry price over time.',\n",
        "    'LoRA adapters enable efficient fine-tuning.',\n",
        "    'I keep a jar of homemade pesto for pasta.',\n",
        "    'New Delhi serves as the seat of India’s government.',\n",
        "    'I like to cook Italian dishes on Sundays.',\n",
        "    'Roasting vegetables caramelizes natural sugars.',\n",
        "    'ETFs provide broad market exposure with intraday liquidity.',\n",
        "    'Proofing time affects a bread’s crumb structure.'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2de82112",
      "metadata": {
        "id": "2de82112"
      },
      "source": [
        "## Predição dos Embeddings\n",
        "\n",
        "Utilize o modelo BERT pré-treinado para gerar embeddings de todos os textos fornecidos.  \n",
        "O objetivo é obter uma matriz `X` com formato **(N, dim)**, onde **N** é o número de textos e **dim** é a dimensionalidade dos vetores de embedding."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers torch scikit-learn matplotlib seaborn pandas umap-learn -q"
      ],
      "metadata": {
        "id": "0bGABFZPylD8"
      },
      "id": "0bGABFZPylD8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "zs78ToiBBtJk"
      },
      "id": "zs78ToiBBtJk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "8vgRHD5EB2LH"
      },
      "id": "8vgRHD5EB2LH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "print(f\"Dimensão do embedding: {model.get_sentence_embedding_dimension()}\")"
      ],
      "metadata": {
        "id": "F3dIVmK_B9Yl"
      },
      "id": "F3dIVmK_B9Yl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "print(f\"Shape da matriz de embeddings: {sentence_embeddings.shape}\")\n",
        "print(f\"Embedding da primeira sentença:\\n{sentence_embeddings[0][:10]}...\")"
      ],
      "metadata": {
        "id": "FZ-PkJs9CFEA"
      },
      "id": "FZ-PkJs9CFEA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f6f497cb",
      "metadata": {
        "id": "f6f497cb"
      },
      "source": [
        "## PCA\n",
        "\n",
        "Aplique **PCA (Principal Component Analysis)** para projetar os embeddings em duas dimensões e visualizar a estrutura global dos dados.  \n",
        "O PCA ajuda a capturar as direções de maior variância e pode indicar agrupamentos lineares.\n",
        "\n",
        "**Tarefas:**\n",
        "- Reduza a dimensionalidade dos embeddings para 2 componentes principais.  \n",
        "- Plote os pontos resultantes com `matplotlib`, identificando possíveis agrupamentos.  \n",
        "- Analise qualitativamente se há separação entre textos de temas distintos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PCA:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.components = None\n",
        "        self.mean = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        # 1. Centralizar os dados (subtrair a média)\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "        X_centered = X - self.mean\n",
        "\n",
        "        # 2. Calcular a matriz de covariância\n",
        "        # rowvar=False indica que as colunas são as variáveis\n",
        "        cov_matrix = np.cov(X_centered, rowvar=False)\n",
        "\n",
        "        # 3. Calcular autovetores e autovalores\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "        # 4. Ordenar autovetores pelos autovalores em ordem decrescente\n",
        "        # Os autovetores são as colunas da matriz `eigenvectors`\n",
        "        eigenvectors = eigenvectors.T\n",
        "        idxs = np.argsort(eigenvalues)[::-1]\n",
        "        eigenvalues = eigenvalues[idxs]\n",
        "        eigenvectors = eigenvectors[idxs]\n",
        "\n",
        "        # 5. Armazenar os `n_components` primeiros autovetores\n",
        "        self.components = eigenvectors[0:self.n_components]\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Centralizar os dados usando a média do treino\n",
        "        X_centered = X - self.mean\n",
        "\n",
        "        # Projetar os dados nos componentes\n",
        "        # (n_samples, n_features) @ (n_features, n_components) -> (n_samples, n_components)\n",
        "        X_projected = np.dot(X_centered, self.components.T)\n",
        "\n",
        "        return X_projected\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)"
      ],
      "metadata": {
        "id": "sILElqtjFT9q"
      },
      "id": "sILElqtjFT9q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Instanciar e aplicar o PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_embeddings = pca.fit_transform(sentence_embeddings)\n",
        "\n",
        "print(\"Dimensão dos dados originais:\", sentence_embeddings.shape)\n",
        "print(\"Dimensão dos dados transformados:\", pca_embeddings.shape)"
      ],
      "metadata": {
        "id": "qk6n-bCWFaSV"
      },
      "id": "qk6n-bCWFaSV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Padronizando as características\n",
        "scaler = StandardScaler()\n",
        "pca_embeddings_scaled = scaler.fit_transform(pca_embeddings)\n",
        "\n",
        "# A média de cada coluna agora é próxima de 0\n",
        "print(\"Média após padronização:\", np.mean(pca_embeddings_scaled, axis=0))\n",
        "# O desvio padrão de cada coluna agora é próximo de 1\n",
        "print(\"Desvio padrão após padronização:\", np.std(pca_embeddings_scaled, axis=0))"
      ],
      "metadata": {
        "id": "l_SqODT3GerG"
      },
      "id": "l_SqODT3GerG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=pca_embeddings_scaled[:, 0], y=pca_embeddings_scaled[:, 1])\n",
        "plt.title('PCA do Embeddings (Escalados)')\n",
        "plt.xlabel('Primeiro Componente Principal (PC1)')\n",
        "plt.ylabel('Segundo Componente Principal (PC2)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y5UlVlLoH0aY"
      },
      "id": "y5UlVlLoH0aY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "25bebfa9",
      "metadata": {
        "id": "25bebfa9"
      },
      "source": [
        "## t-SNE\n",
        "\n",
        "Use **t-SNE (t-distributed Stochastic Neighbor Embedding)** para investigar a estrutura local dos dados.  \n",
        "Diferente do PCA, o t-SNE tenta preservar vizinhanças locais e pode revelar grupos mais sutis.\n",
        "\n",
        "**Tarefas:**\n",
        "- Reduza os embeddings para 2D usando `TSNE` do `scikit-learn`.  \n",
        "- Ajuste parâmetros como `perplexity` e `learning_rate` para comparar resultados.  \n",
        "- Visualize o mapa e observe se os textos semelhantes ficam próximos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_embeddings = tsne.fit_transform(sentence_embeddings)\n",
        "\n",
        "print(\"Dimensão dos dados originais:\", sentence_embeddings.shape)\n",
        "print(\"Dimensão dos dados transformados:\", tsne_embeddings.shape)"
      ],
      "metadata": {
        "id": "MZ5x-550yf9c",
        "collapsed": true
      },
      "id": "MZ5x-550yf9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSNE:\n",
        "    def __init__(self, n_components=2, perplexity=30.0, lr=200.0, n_iter=500):\n",
        "        # Número de dimensões finais, perplexidade, taxa de aprendizado e iterações\n",
        "        self.n_components = n_components\n",
        "        self.perplexity = perplexity\n",
        "        self.lr = lr\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # 1. Calcula matriz de distâncias euclidianas ao quadrado\n",
        "    # -------------------------------------------------------\n",
        "    def _distances(self, X):\n",
        "        sum_X = np.sum(X**2, axis=1)\n",
        "        return np.add(np.add(-2 * X @ X.T, sum_X).T, sum_X)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 2. Calcula a matriz P (probabilidades no espaço de alta dimensão)\n",
        "    #    Cada linha P[i] é ajustada para ter uma entropia que corresponda à perplexidade\n",
        "    # -------------------------------------------------------------------------\n",
        "    def _p_matrix(self, D):\n",
        "        n = D.shape[0]\n",
        "        P = np.zeros((n, n))\n",
        "        logU = np.log2(self.perplexity)\n",
        "\n",
        "        for i in range(n):\n",
        "            beta = 1.0  # inverso da variância (1 / (2*sigma²))\n",
        "            Di = np.delete(D[i], i)  # remove a distância com ele mesmo\n",
        "\n",
        "            # Ajuste de beta por busca simples até alcançar a perplexidade desejada\n",
        "            for _ in range(30):\n",
        "                P_i = np.exp(-Di * beta)\n",
        "                P_i /= np.sum(P_i)\n",
        "                H = -np.sum(P_i * np.log2(P_i + 1e-12))  # entropia\n",
        "                if abs(H - logU) < 1e-3:  # se já está próximo, para\n",
        "                    break\n",
        "                beta *= 1.2 if H > logU else 0.8  # ajusta beta\n",
        "\n",
        "            P[i, np.arange(n) != i] = P_i\n",
        "\n",
        "        # Simetriza e normaliza\n",
        "        P = (P + P.T) / (2 * n)\n",
        "        return np.maximum(P, 1e-12)\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # 3. Executa o t-SNE e retorna o embedding 2D\n",
        "    # -------------------------------------------------------\n",
        "    def fit_transform(self, X):\n",
        "        n = X.shape[0]\n",
        "\n",
        "        # Calcula afinidades no espaço original (P)\n",
        "        D = self._distances(X)\n",
        "        P = self._p_matrix(D)\n",
        "\n",
        "        # Inicializa o embedding aleatoriamente\n",
        "        Y = np.random.randn(n, self.n_components)\n",
        "\n",
        "        # Loop de otimização\n",
        "        for it in range(self.n_iter):\n",
        "            # Calcula afinidades no espaço de baixa dimensão (Q)\n",
        "            sum_Y = np.sum(Y**2, axis=1)\n",
        "            num = 1 / (1 + np.add(np.add(-2 * Y @ Y.T, sum_Y).T, sum_Y))\n",
        "            np.fill_diagonal(num, 0)\n",
        "            Q = np.maximum(num / np.sum(num), 1e-12)\n",
        "\n",
        "            # Calcula o gradiente\n",
        "            PQ = P - Q\n",
        "            for i in range(n):\n",
        "                # Soma ponderada das diferenças (força de atração/repulsão)\n",
        "                dY_i = np.sum((PQ[:, i] * num[:, i])[:, None] * (Y[i] - Y), axis=0)\n",
        "                Y[i] -= self.lr * dY_i  # Atualiza posição\n",
        "\n",
        "            # Centraliza os pontos\n",
        "            Y -= np.mean(Y, axis=0)\n",
        "\n",
        "            # Mostra custo a cada 100 iterações\n",
        "            if (it + 1) % 100 == 0:\n",
        "                cost = np.sum(P * np.log(P / Q))\n",
        "                print(f\"Iter {it+1:4d}: cost = {cost:.4f}\")\n",
        "\n",
        "        return Y"
      ],
      "metadata": {
        "id": "wVaawSxFL72Y"
      },
      "id": "wVaawSxFL72Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tsne = TSNE(n_components=2, perplexity=30.0, n_iter=1000, lr=200)\n",
        "X_embedded_custom = custom_tsne.fit_transform(tsne_embeddings)"
      ],
      "metadata": {
        "id": "evKPeRWINYpY"
      },
      "id": "evKPeRWINYpY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=X_embedded_custom[:, 0],\n",
        "    y=X_embedded_custom[:, 1]\n",
        ")\n",
        "plt.title('Visualização t-SNE com Implementação Customizada')\n",
        "plt.xlabel('Componente t-SNE 1')\n",
        "plt.ylabel('Componente t-SNE 2')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EMvMR4bJNt2M"
      },
      "id": "EMvMR4bJNt2M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4f7829b7",
      "metadata": {
        "id": "4f7829b7"
      },
      "source": [
        "## UMAP\n",
        "\n",
        "Aplique **UMAP (Uniform Manifold Approximation and Projection)** como alternativa ao t-SNE.  \n",
        "O UMAP é mais eficiente, preserva parte da estrutura global e é útil para visualização e pré-processamento.\n",
        "\n",
        "**Tarefas:**\n",
        "- Gere uma projeção 2D dos embeddings com `umap.UMAP`.  \n",
        "- Experimente variar `n_neighbors` e `min_dist` para observar mudanças na distribuição dos clusters.  \n",
        "- Compare visualmente com os resultados do PCA e t-SNE."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "class UMAP:\n",
        "    def __init__(self, n_neighbors=15, n_components=2, n_epochs=200, lr=0.5, random_state=None):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.n_components = n_components\n",
        "        self.n_epochs = n_epochs\n",
        "        self.lr = lr\n",
        "        self.random_state = np.random.RandomState(random_state)\n",
        "        self.a, self.b = 1.929, 0.7915  # parâmetros da função de afinidade do UMAP\n",
        "\n",
        "    def _find_sigma(self, distances, rho):\n",
        "        \"\"\"Busca binária para encontrar sigma.\"\"\"\n",
        "        target = np.log2(self.n_neighbors)\n",
        "        low, high = 1e-3, 10.0\n",
        "        for _ in range(30):\n",
        "            mid = (low + high) / 2\n",
        "            psum = np.sum(np.exp(-(np.maximum(0, distances - rho)) / mid))\n",
        "            if abs(psum - target) < 1e-5:\n",
        "                return mid\n",
        "            if psum > target:\n",
        "                high = mid\n",
        "            else:\n",
        "                low = mid\n",
        "        return mid\n",
        "\n",
        "    def _build_neighbor_graph(self, X):\n",
        "        \"\"\"Etapa 1: constrói o grafo de vizinhança baseado em distâncias.\"\"\"\n",
        "        n = X.shape[0]\n",
        "\n",
        "        # Encontra os k vizinhos mais próximos de cada ponto\n",
        "        knn = NearestNeighbors(n_neighbors=self.n_neighbors).fit(X)\n",
        "        dists, inds = knn.kneighbors(X)\n",
        "\n",
        "        # Calcular rho e sigma\n",
        "        # rho_i: distância mínima não nula (define o raio local de cada ponto)\n",
        "        # sigma_i: largura da distribuição local (ajustada por busca binária)\n",
        "        rho = dists[:, 1]\n",
        "        sigma = np.zeros(n)\n",
        "        for i in range(n):\n",
        "            sigma[i] = self._find_sigma(dists[i, 1:], rho[i])\n",
        "\n",
        "        # Cria pares (i, j) para todas as conexões entre ponto i e seus vizinhos j\n",
        "        rows = np.repeat(np.arange(n), self.n_neighbors)\n",
        "        cols = inds.flatten()\n",
        "\n",
        "        # Calcula distâncias entre cada ponto e seus vizinhos correspondentes\n",
        "        dist = np.linalg.norm(X[rows] - X[cols], axis=1)\n",
        "\n",
        "        # Subtrai o raio local rho_i e zera valores negativos\n",
        "        dist = np.maximum(0, dist - rho[rows])\n",
        "\n",
        "        # Converte distâncias em probabilidades (função de afinidade exponencial)\n",
        "        p = np.exp(-dist / (sigma[rows] + 1e-8))\n",
        "\n",
        "        # União fuzzy (p_ij + p_ji - p_ij*p_ji)\n",
        "        p_matrix = np.zeros((n, n))\n",
        "        p_matrix[rows, cols] = p\n",
        "        p = p_matrix + p_matrix.T - p_matrix * p_matrix.T\n",
        "\n",
        "        # Extrai apenas pares conectados e normaliza os pesos finais\n",
        "        rows, cols = np.where(p > 0)\n",
        "        p = p[rows, cols]\n",
        "        p /= (p.max() + 1e-8)\n",
        "        return rows, cols, p\n",
        "\n",
        "    def _optimize_embedding(self, rows, cols, weights, Y):\n",
        "        \"\"\"Etapa 2: otimiza as posições no espaço reduzido.\"\"\"\n",
        "        eps = 1e-8\n",
        "        for _ in range(self.n_epochs):\n",
        "            # Atração entre vizinhos reais\n",
        "            i = self.random_state.randint(0, len(weights), len(Y))\n",
        "            j = cols[i]\n",
        "            diff = Y[rows[i]] - Y[j]\n",
        "            dist2 = np.sum(diff**2, axis=1)\n",
        "            grad = (2 * self.a * self.b * (dist2 + eps)**(self.b - 1) /\n",
        "                    (1 + self.a * (dist2 + eps)**self.b))[:, None] * diff\n",
        "            Y[rows[i]] -= self.lr * grad\n",
        "            Y[j] += self.lr * grad\n",
        "\n",
        "            # Repulsão entre pares aleatórios\n",
        "            neg_i = self.random_state.randint(0, len(Y), len(Y))\n",
        "            neg_j = self.random_state.randint(0, len(Y), len(Y))\n",
        "            diff = Y[neg_i] - Y[neg_j]\n",
        "            dist2 = np.sum(diff**2, axis=1)\n",
        "            grad = (-self.b / (1 + self.a * (dist2 + eps)**self.b))[:, None] * diff\n",
        "            Y[neg_i] -= self.lr * grad\n",
        "\n",
        "        return Y\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"Executa o UMAP simplificado.\"\"\"\n",
        "        rows, cols, weights = self._build_neighbor_graph(X)\n",
        "        Y = self.random_state.normal(scale=0.01, size=(X.shape[0], self.n_components))\n",
        "        return self._optimize_embedding(rows, cols, weights, Y)"
      ],
      "metadata": {
        "id": "FR0bVm75OwY9"
      },
      "id": "FR0bVm75OwY9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = sentence_embeddings.shape[0]\n",
        "knn = NearestNeighbors(n_neighbors=3).fit(sentence_embeddings)\n",
        "dists, inds = knn.kneighbors(sentence_embeddings)\n",
        "\n",
        "rho = dists[:, 1]\n",
        "sigma = np.zeros(n)\n",
        "\n",
        "rows = np.repeat(np.arange(n), 3)\n",
        "cols = inds.flatten()\n",
        "\n",
        "dist = np.linalg.norm(sentence_embeddings[rows] - sentence_embeddings[cols], axis=1)\n",
        "dist = np.maximum(0, dist - rho[rows])\n",
        "p = np.exp(-dist / (sigma[rows] + 1e-8))\n",
        "\n",
        "p_matrix = np.zeros((n, n))\n",
        "p_matrix[rows, cols] = p\n",
        "\n",
        "p_matrix.shape"
      ],
      "metadata": {
        "id": "Yvfyz_x2PR9G"
      },
      "id": "Yvfyz_x2PR9G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar e rodar o UMAP\n",
        "simple_umap = UMAP(n_neighbors=30, n_components=2, n_epochs=500, random_state=42)\n",
        "embedding = simple_umap.fit_transform(sentence_embeddings)"
      ],
      "metadata": {
        "id": "jB5YZn-fP0Us"
      },
      "id": "jB5YZn-fP0Us",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotar o embedding 2D\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], s=50)\n",
        "plt.title(\"Embedding 2D do Swiss Roll com SimpleUMAP\")\n",
        "plt.xlabel(\"Componente UMAP 1\")\n",
        "plt.ylabel(\"Componente UMAP 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwNfIdsGP6yj"
      },
      "id": "bwNfIdsGP6yj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variando o n_neighbors e o min_dist:"
      ],
      "metadata": {
        "id": "osr2JjuIL5mL"
      },
      "id": "osr2JjuIL5mL"
    },
    {
      "cell_type": "code",
      "source": [
        "n_neighbors_values = [5, 15, 30]\n",
        "min_dist_values = [0.1, 0.5]\n",
        "\n",
        "print(f\"n_neighbors_values: {n_neighbors_values}\")\n",
        "print(f\"min_dist_values: {min_dist_values}\")"
      ],
      "metadata": {
        "id": "I8XSVIV4KfVA"
      },
      "id": "I8XSVIV4KfVA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap.umap_ as umap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "plot_num = 1\n",
        "\n",
        "for n_neighbors in n_neighbors_values:\n",
        "    for min_dist in min_dist_values:\n",
        "        umap_reducer = umap.UMAP(n_components=2, n_neighbors=n_neighbors, min_dist=min_dist)\n",
        "        umap_embeddings = umap_reducer.fit_transform(sentence_embeddings)\n",
        "\n",
        "        plt.subplot(len(n_neighbors_values), len(min_dist_values), plot_num)\n",
        "        sns.scatterplot(x=umap_embeddings[:, 0], y=umap_embeddings[:, 1], s=20)\n",
        "        plt.title(f'UMAP (n_neighbors={n_neighbors}, min_dist={min_dist})')\n",
        "        plt.xlabel('UMAP 1')\n",
        "        plt.ylabel('UMAP 2')\n",
        "        plt.grid(True)\n",
        "        plot_num += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oYXJFqTxLX43"
      },
      "id": "oYXJFqTxLX43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ea498c4f",
      "metadata": {
        "id": "ea498c4f"
      },
      "source": [
        "## Classificação\n",
        "\n",
        "Com base nas categorias observadas nos gráficos anteriores, crie uma função simples que receba um texto e classifique-o na categoria mais provável.\n",
        "\n",
        "**Tarefas:**\n",
        "- Use os embeddings existentes e os clusters identificados para rotular automaticamente cada texto.  \n",
        "- Crie uma função `classificar_texto(texto: str)` que:\n",
        "  1. Gere o embedding do texto.\n",
        "  2. Calcule a distância para os clusters identificados.\n",
        "  3. Retorne o nome do cluster mais próximo."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 1. Realizar o agrupamento KMeans para definir os clusters e seus centroides\n",
        "# Assumindo 5 clusters para começar, baseado nas visualizações anteriores.\n",
        "# Você pode ajustar este número se tiver uma expectativa diferente.\n",
        "num_clusters = 5\n",
        "kmeans_model = KMeans(n_clusters=num_clusters, random_state=42, n_init='auto')\n",
        "kmeans_model.fit(sentence_embeddings)\n",
        "\n",
        "cluster_labels = kmeans_model.labels_  # Rótulos para cada sentença original\n",
        "cluster_centroids = kmeans_model.cluster_centers_  # Centroides dos clusters\n",
        "\n",
        "def classificar_texto(texto: str, model, cluster_centroids, cluster_labels, original_sentences) -> str:\n",
        "       # 1. Gerar o embedding do texto de entrada\n",
        "    text_embedding = model.encode([texto])\n",
        "\n",
        "    # 2. Calcular a distância (similaridade de cosseno) para cada centroide de cluster\n",
        "    # Usamos 1 - cosine_similarity para obter uma distância, onde 0 é mais próximo.\n",
        "    distances = 1 - cosine_similarity(text_embedding, cluster_centroids)\n",
        "\n",
        "    # 3. Encontrar o cluster mais próximo\n",
        "    closest_cluster_idx = np.argmin(distances)\n",
        "\n",
        "    # Por simplicidade, vamos encontrar a sentença original mais próxima do centroide para dar um 'nome' descritivo.\n",
        "    # Primeiro, obtenha os embeddings das sentenças que pertencem ao cluster mais próximo\n",
        "    sentences_in_closest_cluster_indices = np.where(cluster_labels == closest_cluster_idx)[0]\n",
        "    embeddings_in_closest_cluster = sentence_embeddings[sentences_in_closest_cluster_indices]\n",
        "\n",
        "    # Calcule a similaridade do texto de entrada com todas as sentenças neste cluster\n",
        "    similarities_to_cluster_sentences = cosine_similarity(text_embedding, embeddings_in_closest_cluster)\n",
        "\n",
        "    # Encontre a sentença mais similar dentro do cluster para dar um nome descritivo\n",
        "    most_similar_sentence_idx_in_cluster = np.argmax(similarities_to_cluster_sentences)\n",
        "    original_idx = sentences_in_closest_cluster_indices[most_similar_sentence_idx_in_cluster]\n",
        "    cluster_name = original_sentences[original_idx]\n",
        "\n",
        "    return f\"Cluster {closest_cluster_idx}: (Representado por: '{cluster_name}')\""
      ],
      "metadata": {
        "id": "deLw-w1wSrqU"
      },
      "id": "deLw-w1wSrqU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0708a279"
      },
      "source": [
        "import random\n",
        "\n",
        "# Escolher um texto aleatório da lista de sentenças\n",
        "random_text = random.choice(sentences)\n",
        "\n",
        "# Classificar o texto aleatório\n",
        "categoria_aleatoria = classificar_texto(random_text, model, cluster_centroids, cluster_labels, sentences)\n",
        "\n",
        "print(f\"O texto aleatório selecionado é: '{random_text}'\")\n",
        "print(f\"Ele pertence à categoria: {categoria_aleatoria}\")"
      ],
      "id": "0708a279",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82a5b93b"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1. Gerar o embedding do texto de entrada (apenas 10 elementos para nao ficar grande)\n",
        "random_text_embedding = model.encode([random_text])\n",
        "print(f\"Embedding do texto de exemplo (primeiros 10 elementos):\\n{random_text_embedding[0][:10]}...\")\n",
        "\n",
        "# 2. Calcular a distância (similaridade de cosseno) para cada centroide de cluster\n",
        "# Usamos 1 - cosine_similarity para obter uma 'distância', onde 0 é mais próximo.\n",
        "# A função cosine_similarity retorna valores entre -1 (totalmente oposto) e 1 (totalmente similar).\n",
        "# Portanto, 1 - sim resultará em valores entre 0 (similar) e 2 (oposto).\n",
        "distances_to_centroids = 1 - cosine_similarity(random_text_embedding, cluster_centroids)\n",
        "\n",
        "print(f\"\\nDistâncias calculadas para cada centroide de cluster:\\n{distances_to_centroids}\")\n",
        "\n",
        "# 3. Encontrar o cluster mais próximo\n",
        "closest_cluster_idx = np.argmin(distances_to_centroids)\n",
        "closest_distance = distances_to_centroids[0, closest_cluster_idx]\n",
        "print(f\"A menor distância é: {closest_distance:.5f}\")"
      ],
      "id": "82a5b93b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac00809b"
      },
      "source": [
        "print(f\"Texto aleatório selecionado: '{random_text}'\")\n",
        "\n",
        "text_embedding_rand = model.encode([random_text])\n",
        "\n",
        "similarities_to_centroids = cosine_similarity(text_embedding_rand, cluster_centroids)[0]\n",
        "\n",
        "closest_cluster_idx_rand = np.argmax(similarities_to_centroids)\n",
        "\n",
        "# Uma cópia para não modificar o array original\n",
        "temp_similarities = np.copy(similarities_to_centroids)\n",
        "temp_similarities[closest_cluster_idx_rand] = -np.inf # Define como -infinito para ignorar\n",
        "\n",
        "second_closest_cluster_idx_rand = np.argmax(temp_similarities)\n",
        "second_closest_similarity_rand = temp_similarities[second_closest_cluster_idx_rand]\n",
        "\n",
        "sentences_in_second_closest_cluster_indices = np.where(cluster_labels == second_closest_cluster_idx_rand)[0]\n",
        "embeddings_in_second_closest_cluster = sentence_embeddings[sentences_in_second_closest_cluster_indices]\n",
        "similarities_to_cluster_sentences = cosine_similarity(text_embedding_rand, embeddings_in_second_closest_cluster)\n",
        "\n",
        "most_similar_sentence_idx_in_cluster = np.argmax(similarities_to_cluster_sentences)\n",
        "original_idx = sentences_in_second_closest_cluster_indices[most_similar_sentence_idx_in_cluster]\n",
        "second_closest_cluster_name = sentences[original_idx]\n",
        "\n",
        "print(f\"O cluster mais próximo é: Cluster {second_closest_cluster_idx_rand} (Similaridade: {second_closest_similarity_rand:.4f})\")\n",
        "print(f\"Este cluster é representado por: '{second_closest_cluster_name}'\")"
      ],
      "id": "ac00809b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}